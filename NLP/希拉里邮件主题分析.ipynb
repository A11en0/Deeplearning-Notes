{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import doc2vec, ldamodel\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                  ExtractedBodyText\n",
      "1   2  B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...\n",
      "2   3                                                Thx\n",
      "4   5  H <hrod17@clintonemail.com>\\nFriday, March 11,...\n",
      "5   6  Pis print.\\n-•-...-^\\nH < hrod17@clintonernail...\n",
      "7   8  H <hrod17@clintonemail.corn>\\nFriday, March 11...\n",
      "(6742, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./resources/HillaryEmails.csv\")\n",
    "df = df[['Id', 'ExtractedBodyText']].dropna()\n",
    "print(df.head())\n",
    "print(df.shape)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email_text(text):\n",
    "    # 数据清洗\n",
    "    text = text.replace('\\n', \" \")  # 新行，我们是不需要的\n",
    "    text = re.sub(r\"-\", \" \", text)  # 把 \"-\" 的两个单词，分开。（比如：july-edu ==> july edu）\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text)  # 日期，对主体模型没什么意义\n",
    "    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text)  # 时间，没意义\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text)  # 邮件地址，没意义\n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text)  # 网址，没意义\n",
    "    pure_text = ''\n",
    "    # 以防还有其他特殊字符（数字）等等，我们直接把他们loop一遍，过滤掉\n",
    "    for letter in text:\n",
    "        # 只留下字母和空格\n",
    "        if letter.isalpha() or letter == ' ':\n",
    "            pure_text += letter\n",
    "    # 再把那些去除特殊字符后落单的单词，直接排除。\n",
    "    # 我们就只剩下有意义的单词了。\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word) > 1)  # 而且单词长度必须是2以上\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       Thursday March PM Latest How Syria is aiding Q...\n",
      "2                                                     Thx\n",
      "4       Friday March PM Huma Abedin Fw Latest How Syri...\n",
      "5       Pis print Wednesday September PM Fw Meet The R...\n",
      "7       Friday March PM Huma Abedin Fw Latest How Syri...\n",
      "                              ...                        \n",
      "7938    Hi Sorry havent had chance to see you but did ...\n",
      "7939    assume you saw this by now if not its worth re...\n",
      "7941    Big change of plans in the Senate Senator Reid...\n",
      "7943    PVerveer Friday December AM From Please let me...\n",
      "7944                                            See below\n",
      "Name: ExtractedBodyText, Length: 6742, dtype: object\n"
     ]
    }
   ],
   "source": [
    "docs = df['ExtractedBodyText']\n",
    "docs = docs.apply(lambda s: clean_email_text(s))\n",
    "\n",
    "# print(docs.head(1).values)\n",
    "\n",
    "doclist = docs.values\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword():\n",
    "    stopword = []\n",
    "    with open('./resources/stopwords.txt', 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.replace('\\n', '')\n",
    "            stopword.append(line)\n",
    "    return stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thursday', 'march', 'pm', 'latest', 'syria', 'aiding', 'qaddafi', 'sid', 'hrc', 'memo', 'syria', 'aiding', 'libya', 'docx', 'hrc', 'memo', 'syria', 'aiding', 'libya', 'docx', 'march', 'hillary']\n"
     ]
    }
   ],
   "source": [
    "stop_word = remove_stopword()\n",
    "texts = [[word for word in doc.lower().split() if word not in stop_word] for doc in doclist]\n",
    "print(texts[0])  # 第一个文本现在的样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (1, 2), (2, 1), (3, 2), (4, 1), (5, 2), (6, 2), (7, 2), (8, 1), (9, 1), (10, 1), (11, 3), (12, 1)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006*\"obama\" + 0.005*\"american\" + 0.005*\"president\" + 0.005*\"time\" + 0.005*\"government\"\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)\n",
    "print(lda.print_topic(10, topn=5))  # 第10个主题最关键的五个词\n",
    "# pprint(lda.print_topics(num_topics=20, num_words=3))  # 所有的主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.save(\"lda.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.29656535), (4, 0.12008975), (10, 0.27462456), (13, 0.1942672)]\n"
     ]
    }
   ],
   "source": [
    "lda = ldamodel.LdaModel.load(\"lda.model\")\n",
    "text = \"I was greeted by this heartwarming display on the corner of my street today. Thank you to all of you who did this. Happy Thanksgiving. -H\"\n",
    "text = clean_email_text(text)\n",
    "\n",
    "texts = [word for word in text.lower().split() if word not in stop_word]\n",
    "bow = dictionary.doc2bow(texts)\n",
    "print(lda.get_document_topics(bow))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
